# Hash Table

**散列表借助“数组支持按照下标随机访问时时间复杂度为 $O(1)$”的特性**，

插入：通过散列函数把元素的键 (key) 映射为下标(散列值)，然后将对象(包括键 key 和值 value) 存储在数组对应下标的位置；

查询：也是先通过散列函数将 key 映射为散列值然后在对应的位置找需要的对象，由于多个 key 可能对应一个散列值，此时还需要再对比 key 才行。

散列表中数据是经过散列函数打乱之后无规律存储的。

## 散列函数

散列函数(假设为 $hash(key)$)设计的基本要求：

1. 散列函数计算得到的散列值是一个非负整数；
2. 如果 key1 = key2，那 $hash(key1) == hash(key2)$；
3. 如果 key1 ≠ key2，那 $hash(key1) ≠ hash(key2)$。

理想情况下，散列函数需要保证任何两个不同的 key 得到的散列值不同，而由于空间限制，实际中这是不可能的，也就是说无法完全避免散列冲突(也叫哈希碰撞)，通常需要**保证散列值尽可能随机且均匀分布**。

那么，该如何尽可能地减少散列冲突呢？有以下两种方法：

### 开放寻址法(Open Addressing)

如果出现了散列冲突，就重新探测一个空闲位置，将其插入。

所谓开地址法也是指某个元素的位置并不永远由其哈希值决定。

优点：数据都存储在数组中，可以有效地利用 CPU 缓存加快查询速度。且这种方法实现的散列表，序列化起来比较简单。

缺点：删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据(见下面)。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以装载因子的上限不能太大，这也导致这种方法比链表法更浪费内存空间。

**当数据量比较小、装载因子(见下面)小的时候，适合采用开放寻址法。这也是 Java 中的 `ThreadLocalMap` 使用开放寻址法解决散列冲突的原因。**

有以下集中探测空闲位置的方式：

- 线性探测(Linear Probing)

插入操作：如果某-个数据经过散列函数散列之后，存储位置已经被占用了，就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。

查找操作：类似于插入，通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。

删除操作：有些特别，不能单纯地把要删除的元素设置为空，因为在查找的时候，一旦我们通过线性探测方法，找到一个空闲位置，就可以认定散列表中不存在这个数据。但是，如果这个空闲位置是我们后来删除的，就会导致原来的查找算法失效。本来存在的数据，会被认定为不存在。该如何解决呢？

可以将删除的元素，特殊标记为 deleted。当线性探测查找的时候，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测。

弊端：当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为 $O(n)$。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据。

- 二次探测(Quadratic probing)

类似于线性探测，线性探测每次探测的步长是 1，那它探测的下标序列就是 $hash(key)+0，hash(key)+1，hash(key)+2……$，而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是 $hash(key)+0，hash(key)+1^2，hash(key)+2^2……$。

- 双重散列(Double hashing)

使用一组散列函数 $hash1(key)$，$hash2(key)$，$hash3(key)……$，我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。

### 链表法(Separate Chaining)

相比于开放寻址法要简单很多。在散列表中，每个桶(bucket)或槽(slot)对应一条链表，所有散列值相同的元素会被放到相同槽位对应的链表中。

插入操作：通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，故时间复杂度是 $O(1)$；

查找、删除操作：通过散列函数计算出对应的槽，然后遍历链表查找或者删除。这两个操作的时间复杂度跟链表的长度 k 成正比，也就是 $O(k)$。对于散列比较均匀的散列函数来说，理论上讲，$k=n/m$，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。

- 优点：
  - 对内存的利用率比开放寻址法要高。可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。这也是链表优于数组的地方。
  - 对大装载因子的容忍度更高。只要散列函数的值随机均匀，即便装载因子变成 10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。

**基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。JDK1.8 后的 `HashMap` 就引入了红黑树，详见 Java 中笔记。**

### 装载因子

**使用装载因子(load factor)表示空闲位置的多少，即：$装载因子=填入表中的元素个数/散列表的长度$，装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。**

当装载因子过大时，可以进行动态扩容，重新申请一个更大的散列表将数据搬移到这个新散列表中。注意，需要根据散列函数重新计算散列值再存储。

如果我们对空间消耗非常敏感，我们可以在装载因子小于某个值之后，启动动态缩容。

装载因子阈值需要选择得当。如果太大，会导致冲突过多；如果太小，会导致内存浪费严重。

- 如何避免低效扩容？

直接服务于用户的业务代码，尽管大部分情况下，插入一个数据的操作都很快，但是，极个别非常慢的插入操作，也会让用户崩溃。这个时候，“一次性”扩容的机制就不合适了。如：

假设散列表当前大小为 1GB，要想扩容为原来的两倍大小，那就需要对 1GB 的数据重新计算哈希值，并且从原来的散列表搬移到新的散列表，听起来就很耗时，是不是？

为了解决一次性扩容耗时过多的情况，可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。

当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。

而在这期间的查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。

通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，插入一个数据的时间复杂度都是 O(1)。

### 工业级散列表

工业级的散列表应该具有哪些特性：

- 支持快速的查询、插入、删除操作；
- 内存占用合理，不能浪费过多的内存空间；
- 性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。


## 应用

### 散列表+链表

散列表通常会和链表一起使用，

- LRU 淘汰算法

之前使用[链表实现 LRU 淘汰算法](./02-List.md)，但是其时间复杂度为 $O(n)$，而将链表与散列表组合，可以将时间复杂度将为 $O(1)$。

实现：使用双链表存储数据，链表中每个结点存储了数据(data)、前驱指针(prev)、后继指针(next)、hnext，越靠近链表尾部的结点是越早访问的数据结点。

说明：这种散列表是通过链表法解决散列冲突的，所以每个结点会存在于两个链表中，一个是用于 LRU 算法的双向链表，一个是散列表中的单链表，而 hnext 字段就是为了在散列表中将存在散列冲突的数据串起来的！具体结构：

![](../../images/LRU_hashtable.jpg)

查找操作：散列表查找操作的时间复杂度接近 $O(1)$，所以可以很快地在缓存中找到数据，当找到数据后，还要将其移动到双向链表的头部；

删除操作：$O(1)$ 时间复杂度找到数据对应的结点，而双向链表可以以 $O(1)$ 时间复杂度找到其前结点，所以整个删除操作的时间复杂度也是 $O(1)$；

添加操作：先查看缓存中是否已有该数据，已有的话将其移动到双向链表的头部；没有的话，判断缓存是否已满，已满则将双向链表尾部的结点删除，再将数据放到双向链表的头部，未满则直接将数据放到双向链表的头部。

- Redis 的 zset

Redis 中的有序集合 zset 是使用跳表和散列表实现的，每个成员对象有两个重要的属性，key(键值)和 score(分值)。我们不仅会通过 score 来查找数据，还会通过 key 来查找数据。

其主要的操作有：添加一个成员对象、按照键值来删除一个成员对象、按照键值来查找一个成员对象、按照分值区间查找数据，比如查找积分在 [100, 356] 之间的成员对象、按照分值从小到大排序成员变量；

- Java 中的 LinkedHashMap

Java 中 `LinkedHashMap` 底层是通过散列表和链表组合在一起实现的，不仅支持按照插入顺序遍历数据，还支持按照访问顺序来遍历数据。“Linked” 是只使用双向链表实现。

```java
    HashMap<Integer, Integer> m1 = new LinkedHashMap<>();
    m1.put(3, 11);
    m1.put(1, 12);
    m1.put(5, 23);
    m1.put(2, 22);

    for (Map.Entry e : m1.entrySet()) {
        System.out.println(e.getKey()); // 输出顺序为 3、1、5、2
    }


    // 10是初始大小，0.75是装载因子，true是表示按照访问时间排序
    HashMap<Integer, Integer> m2 = new LinkedHashMap<>(10, 0.75f, true);
    // 每次 put 操作会将数据放到链表尾部
    m2.put(3, 11);
    m2.put(1, 12);
    m2.put(5, 23);
    m2.put(2, 22);
    // 先查看这个 key 是否已有，将已有的 (3,11) 删除，再将 (3.26) 放到链表尾部
    m2.put(3, 26);
    // 会将被访问到的 (5,23) 移动到链表尾部
    m2.get(5);
    // 遍历是从头节点遍历
    for (Map.Entry e : m2.entrySet()) {
        System.out.println(e.getKey()); // 输出顺序为 1、2、3、5
    }
```

可以发现，按照访问时间排序的 `LinkedHashMap` 本身就是一个支持 LRU 缓存淘汰策略的缓存系统。

### 单词拼写检查

Word 文档中单词拼写功能如何实现？

常用的英文单词有 20 万个左右，假设单词的平均长度是 10 个字母，平均一个单词占用 10 个字节的内存空间，那 20 万英文单词大约占 2MB 的存储空间，就算放大 10 倍也就是 20MB。对于现在的计算机来说，这个大小完全可以放在内存里面。

所以我们可以用散列表来存储整个英文单词词典。当用户输入某个英文单词时，我们拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。借助散列表这种数据结构，我们就可以轻松实现快速判断是否存在拼写错误。

### 题目

- 假设有 10 万条 URL 访问日志，如何按照访问次数给 URL 排序？

遍历 10 万条数据，以 URL 为 key，访问次数为 value，存入散列表，同时记录下访问次数的最大值 K，时间复杂度 $O(N)$。

如果 K 不是很大，可以使用桶排序，时间复杂度 $O(N)$。如果 K 非常大（比如大于 10 万），就使用快速排序，复杂度 $O(NlogN)$。

- 有两个字符串数组，每个数组大约有 10 万条字符串，如何快速找出两个数组中相同的字符串？

以第一个字符串数组构建散列表，key 为字符串，value 为出现次数。再遍历第二个字符串数组，以字符串为 key 在散列表中查找，如果 value 大于零，说明存在相同字符串。时间复杂度 $O(N)$。

